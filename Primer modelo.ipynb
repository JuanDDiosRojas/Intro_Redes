{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer modelo\n",
    "Vamos a tomar un conjunto de datos creados de manera sintética y utilizaremos el descenso del gradiente para optimizar un modelo que se ajuste a ellos.\n",
    "Para el caso más sencillo utilizaremos una relación lineal entre las variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supondremos un modelo que depende de dos variables $f(x_1,x_2)=y$ con una relación lineal entre las variables de la manera \n",
    "$y=w_1x_1+w_2x_2+b$\n",
    "\n",
    "Supondremos que partimos de un conjunto de n datos, los cuales llamaremos ejemplos: de la forma \n",
    "$$y_1=w_1x_1^1+w_2x_1^2+b$$\n",
    "$$y_2=w_1x_2^1+w_2x_2^2+b$$\n",
    "$$...$$\n",
    "$$y_n=w_1x_n^1+w_2x_n^2+b$$\n",
    "\n",
    "$$Y=XW+b$$\n",
    "\n",
    "Con Y el vector que contiene a los $y_i$, X y W análogamente contienen a $w_i$ y a $x_i^j$\n",
    "\n",
    "Para entrenar este modelo vamos a generar definir un par de $w_1$ y $w_2$ y les agregaremos ruido aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from descenso import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trabajamos en el dominio [-10,10]\n",
    "#Incluimos a b en w, w=[w1,w2,b]\n",
    "def datos_prueba(w,n):\n",
    "    #n será el número de ejemplos\n",
    "    #generamos el dominio de los datos, aleatorios con distribución normal\n",
    "    X=np.random.normal(-10,10,(n,len(w)-1))\n",
    "    \n",
    "    X1=[]\n",
    "    for i in X:\n",
    "        i=np.append(i,1.0)\n",
    "        X1.append(i)\n",
    "    X1=np.array(X1)\n",
    "    Y = np.dot(X1, w) \n",
    "    Y=Y+np.random.normal(-1.0,0.8,Y.shape)\n",
    "    return X1,Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minibatch\n",
    "Con el fin de ahorrar costo computacional sólo trabajaremos con un porcentaje del número total de ejemplos, a este subconjunto se le llama minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(size,X,Y):\n",
    "    n=len(X)\n",
    "    #tomamos la lista de índices de tamaño n\n",
    "    indices=np.arange(0,n)\n",
    "    indices=list(indices)\n",
    "    #Tomamos una lista aleatoria de índices de tamaño=size\n",
    "    ibatch=random.sample(indices, size)\n",
    "    Xbatch=[]\n",
    "    Ybatch=[]\n",
    "    #Generamos las nuevas listas random\n",
    "    for i in ibatch:\n",
    "        Xbatch.append(X[i])\n",
    "        Ybatch.append(Y[i])\n",
    "        \n",
    "    return np.array(Xbatch),np.array(Ybatch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definiendo la función error\n",
    "Para calcular el error en nuestro modelo de aproximación debemos calcular el error de aproximación por cada ejemplo y para el conjunto completo: la suma de estos.\n",
    "Es decir la función de error para un conjunto de n ejemplos será:\n",
    "$$Error=E(W,b)=\\sum_{i=1}^{n}l_i(W,b)$$\n",
    "\n",
    "Donde $l_i$ será el error de cada ejemplo, en este caso se tomará el error cuadrático medio.\n",
    "Entonces:\n",
    "$$l_i=\\frac{1}{2}(\\bar{y_i}-y_i)^2$$\n",
    "\n",
    "Donde $\\bar{y_i} será el valor estimado por nuestro modelo$\n",
    "entonces para este caso la función error será \n",
    "$$l_i=\\frac{1}{2}((w_1x_1^i+w_2x_2^i+b)-y_i)²$$\n",
    "\n",
    "Y el error total\n",
    "$$E(W,b)=\\sum_{i=1}^{n}\\frac{1}{2}((w_1x_1^i+w_2x_2^i+b)-y_i)²$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificaremos las variables para meter b en W, agregamos b a w\n",
    "y añadimos una columna de \"1\" a X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprendizaje\n",
    "Con ayuda del descenso del gradiente podemos encontrar los valores que nos otorguen el modelo esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=[3,5,7] #w real de la forma [w1,w2,b] \n",
    "X1,Y1=datos_prueba(w,100)# datos sintéticos\n",
    "\n",
    "def error(W):\n",
    "    X,Y=minibatch(80,X1,Y1)\n",
    "    #suma del error medio cuadrático de cada ejemplo\n",
    "    s=0\n",
    "    for i in range(0,len(Y)):\n",
    "        l=0.5*(np.dot(X[i],W)-Y[i])**2\n",
    "        s=s+l\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8c061400e32b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescenso_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/gitHub/misRepos/Intro_Redes/descenso.py\u001b[0m in \u001b[0;36mdescenso_grad\u001b[0;34m(f, X0, eta)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mX0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m10e-8\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "descenso_grad(error,[1,1,1],0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def partial(g,k,X):\n",
    "        h=1e-9\n",
    "        Y=np.copy(X)\n",
    "        X[k-1]=X[k-1]+h\n",
    "        dp=(g(X)-g(Y))/h\n",
    "        return dp\n",
    "    #Ahora definimos la función que nos dará el gradiente\n",
    "def grad(f,X):\n",
    "    grd=[]\n",
    "    for i in np.arange(0,len(X)):\n",
    "        ai=partial(f,i+1,X)\n",
    "        grd.append(ai)\n",
    "    return grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[1,1,1]\n",
    "error(W)\n",
    "#for i in range(1):\n",
    "print(grad(error,W))\n",
    "W=W-0.3*np.array(grad(error,W))\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(error,[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
